{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python libraries required for the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Performance metrics\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# #Classifier imports\n",
    "# from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "# from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from rfpimp import *\n",
    "# from rfpimp import *\n",
    "# from xgboost import XGBClassifier\n",
    "# from xgboost import plot_importance\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# from rfpimp import *\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MACHINEID', 'UPDATETIME', 'GT_BARCODE', 'TIRE_TYPE', 'COMPONENT_TYPE',\n",
      "       'CUT_LENGTH', 'CONV_TIME', 'SPLICE_LENGTH_LEFT', 'SPLICE_LENGTH_MID',\n",
      "       'SPLICE_LENGTH_RIGHT', 'PRESSURE_S1_L', 'PRESSURE_S1_R',\n",
      "       'PRESSURE_S2_L', 'PRESSURE_S2_R', 'PRESSURE_S3_L', 'PRESSURE_S3_R',\n",
      "       'PRESSURE_S4_L', 'PRESSURE_S4_R', 'PRESSURE_S5_L', 'PRESSURE_S5_R',\n",
      "       'PRESSURE_S6_L', 'PRESSURE_S6_R', 'PRESSURE_S7_L', 'PRESSURE_S7_R',\n",
      "       'PRESSURE_S8_L', 'PRESSURE_S8_R', 'COMP_PART', 'COMP_BARCODE',\n",
      "       'COMP_BUILDDATE', 'COMP_MACHINEID', 'COMP_POSITION_BIN',\n",
      "       'LFT_SPLICE_BIN', 'LFT_SPLICE_DELTA', 'LFT_SPLICE_LAG1',\n",
      "       'LFT_SPLICE_MA5', 'LFT_SPLICE_MA10', 'LFT_SPLICE_MA20',\n",
      "       'LFT_SPLICE_MA50', 'LFT_SPLICE_SLOPE5', 'LFT_SPLICE_SLOPE10',\n",
      "       'LFT_SPLICE_SLOPE20', 'LFT_SPLICE_SLOPE50', 'MID_SPLICE_BIN',\n",
      "       'MID_SPLICE_DELTA', 'MID_SPLICE_LAG1', 'MID_SPLICE_MA5',\n",
      "       'MID_SPLICE_MA10', 'MID_SPLICE_MA20', 'MID_SPLICE_MA50',\n",
      "       'MID_SPLICE_SLOPE5', 'MID_SPLICE_SLOPE10', 'MID_SPLICE_SLOPE20',\n",
      "       'MID_SPLICE_SLOPE50', 'RHT_SPLICE_BIN', 'RHT_SPLICE_DELTA',\n",
      "       'RHT_SPLICE_LAG1', 'RHT_SPLICE_MA5', 'RHT_SPLICE_MA10',\n",
      "       'RHT_SPLICE_MA20', 'RHT_SPLICE_MA50', 'RHT_SPLICE_SLOPE5',\n",
      "       'RHT_SPLICE_SLOPE10', 'RHT_SPLICE_SLOPE20', 'RHT_SPLICE_SLOPE50',\n",
      "       'LFT_SPLICE_OK', 'MID_SPLICE_OK', 'RHT_SPLICE_OK', 'SPLICE_OK'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('..\\BridgestoneData\\VMI_BP1_mod01.v1.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CUT_LENGTH', 'CONV_TIME', 'PRESSURE_S1_L', 'PRESSURE_S1_R',\n",
      "       'PRESSURE_S2_L', 'PRESSURE_S2_R', 'PRESSURE_S3_L', 'PRESSURE_S3_R',\n",
      "       'PRESSURE_S4_L', 'PRESSURE_S4_R', 'PRESSURE_S5_L', 'PRESSURE_S5_R',\n",
      "       'COMP_MACHINEID', 'COMP_POSITION_BIN', 'LFT_SPLICE_LAG1',\n",
      "       'LFT_SPLICE_MA5', 'LFT_SPLICE_MA10', 'LFT_SPLICE_MA20',\n",
      "       'LFT_SPLICE_MA50', 'LFT_SPLICE_SLOPE5', 'LFT_SPLICE_SLOPE10',\n",
      "       'LFT_SPLICE_SLOPE20', 'LFT_SPLICE_SLOPE50', 'MID_SPLICE_LAG1',\n",
      "       'MID_SPLICE_MA5', 'MID_SPLICE_MA10', 'MID_SPLICE_MA20',\n",
      "       'MID_SPLICE_MA50', 'MID_SPLICE_SLOPE5', 'MID_SPLICE_SLOPE10',\n",
      "       'MID_SPLICE_SLOPE20', 'MID_SPLICE_SLOPE50', 'RHT_SPLICE_LAG1',\n",
      "       'RHT_SPLICE_MA5', 'RHT_SPLICE_MA10', 'RHT_SPLICE_MA20',\n",
      "       'RHT_SPLICE_MA50', 'RHT_SPLICE_SLOPE5', 'RHT_SPLICE_SLOPE10',\n",
      "       'RHT_SPLICE_SLOPE20', 'RHT_SPLICE_SLOPE50', 'SPLICE_OK'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['MACHINEID', 'UPDATETIME', 'GT_BARCODE', 'TIRE_TYPE', 'COMPONENT_TYPE',\n",
    "              'SPLICE_LENGTH_LEFT',  'SPLICE_LENGTH_MID', 'SPLICE_LENGTH_RIGHT', \n",
    "              'COMP_PART', 'COMP_BARCODE', 'COMP_BUILDDATE',\n",
    "              'LFT_SPLICE_BIN', 'LFT_SPLICE_DELTA', \n",
    "              'MID_SPLICE_BIN', 'MID_SPLICE_DELTA', \n",
    "              'RHT_SPLICE_BIN',  'RHT_SPLICE_DELTA', \n",
    "              'LFT_SPLICE_OK', 'MID_SPLICE_OK', 'RHT_SPLICE_OK',\n",
    "              'PRESSURE_S6_L', 'PRESSURE_S6_R', 'PRESSURE_S7_L', 'PRESSURE_S7_R', 'PRESSURE_S8_L', 'PRESSURE_S8_R',\n",
    "              'COMP_MACHINEID'\n",
    "             ], axis=1)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the columns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Total  Percent\n",
      "SPLICE_OK              0      0.0\n",
      "PRESSURE_S5_L          0      0.0\n",
      "LFT_SPLICE_MA50        0      0.0\n",
      "LFT_SPLICE_MA20        0      0.0\n",
      "LFT_SPLICE_MA10        0      0.0\n",
      "LFT_SPLICE_MA5         0      0.0\n",
      "LFT_SPLICE_LAG1        0      0.0\n",
      "COMP_POSITION_BIN      0      0.0\n",
      "COMP_MACHINEID         0      0.0\n",
      "PRESSURE_S5_R          0      0.0\n"
     ]
    }
   ],
   "source": [
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "print(missing_data.head(10))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['SPLICE_OK'], axis=1)\n",
    "y = df['SPLICE_OK']\n",
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields COMP_MACHINEID",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a884b948704d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# check classification accuracy of XGB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[1;32m--> 585\u001b[1;33m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.n_estimators,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    342\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[0;32m    343\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m                                                                 feature_types)\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[1;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    212\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[0;32m    213\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields COMP_MACHINEID"
     ]
    }
   ],
   "source": [
    "# use train/test split with different random_state values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "\n",
    "# check classification accuracy of XGB\n",
    "xgb = XGBClassifier(n_estimators=250, max_depth=5) \n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# cross-validation with 10 folds\n",
    "# scores = cross_val_score(xgb, X, y, cv=10, scoring='accuracy')\n",
    "# print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use average accuracy as an estimate of out-of-sample accuracy\n",
    "# print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train,y_train)\n",
    "y2_XGB_model = xgb.predict(X_test)\n",
    "\n",
    "# target_names = ['Bad', 'OK', 'Good']\n",
    "target_names = ['Bad', 'Good']\n",
    "class_names = target_names\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y2_XGB_model)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='XGB Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='XGB Normalized confusion matrix')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # search for an optimal value of estimators for XGB\n",
    "# k_range = list(range(25, 151, 25))\n",
    "# k_scores = []\n",
    "# for k in k_range:\n",
    "#     xgb = XGBClassifier(n_estimators=k)\n",
    "#     scores = cross_val_score(xgb, X, y, cv=10, scoring='accuracy')\n",
    "#     k_scores.append(scores.mean())\n",
    "#     print(k, ' -- ', scores)\n",
    "# print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # plot the value of K for XGB (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "# plt.plot(k_range, k_scores)\n",
    "# plt.xlabel('Value of n_estimators for XGB')\n",
    "# plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/justmarkham/scikit-learn-videos/blob/master/07_cross_validation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/justmarkham/scikit-learn-videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/#parameter-tuning-with-cross-validation\n",
    "\n",
    "# # changing to misclassification error\n",
    "# MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# # determining best k\n",
    "# optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "# print \"The optimal number of neighbors is %d\" % optimal_k\n",
    "\n",
    "# # plot misclassification error vs k\n",
    "# plt.plot(neighbors, MSE)\n",
    "# plt.xlabel('Number of Neighbors K')\n",
    "# plt.ylabel('Misclassification Error')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
